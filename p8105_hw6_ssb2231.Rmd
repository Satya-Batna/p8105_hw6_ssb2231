---
title: "P8105 Homework 6"
author: "Satya Batna"
output: 
  github_document:
    toc: true
---
```{r, message= FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.width = 7,
  fig.height = 5,
  out.width = "80%"
)

library(tidyverse)
library(broom)
library(purrr)
library(modelr)
library(p8105.datasets)
library(janitor)

```

**Problem 1**
```{r,echo=FALSE, message=FALSE, warning=FALSE}
homicide_raw = read_csv("data/homicide-data.csv")

homicide_df =
  homicide_raw |>
  mutate(
    city_state = str_c(city, state, sep = ", "),
    
      resolved = case_when(
      disposition == "Closed by arrest" ~ 1,
      TRUE ~ 0
    ),
    resolved = factor(resolved),
    
    victim_race = str_to_lower(victim_race),
        victim_age = as.numeric(victim_age)
  ) |>
  filter(
    !city_state %in% c("Dallas, TX",
                       "Phoenix, AZ",
                       "Kansas City, MO",
                       "Tulsa, AL")
  ) |>
  filter(victim_race %in% c("white", "black"))

homicide_df
```

```{r,  echo=FALSE, message=FALSE, warning=FALSE}
# Filter to Baltimore only
baltimore_df =
  homicide_df |>
  filter(city_state == "Baltimore, MD")

# Logistic regression: resolved vs unresolved
baltimore_fit =
  glm(
    resolved ~ victim_age + victim_sex + victim_race,
    data = baltimore_df,
    family = binomial()
  )

summary(baltimore_fit)

baltimore_tidy =
  tidy(baltimore_fit, conf.int = TRUE, exponentiate = TRUE)

baltimore_tidy
```


```{r,echo=FALSE, message=FALSE, warning=FALSE}
city_or =
  homicide_df |>
  nest(data = -city_state) |>
  mutate(
    fit = map(
      data,
      ~ glm(
          resolved ~ victim_age + victim_sex + victim_race,
          data = .x,
          family = binomial()
        )
    ),
    results = map(
      fit,
      ~ tidy(.x, conf.int = TRUE, exponentiate = TRUE)
    )
  ) |>
  select(city_state, results) |>
  unnest(results) |>
  filter(term == "victim_sexMale") |>
  select(city_state, estimate, conf.low, conf.high)

city_or

# reorder cities by OR
city_or_plot =
  city_or |>
  mutate(city_state = forcats::fct_reorder(city_state, estimate))

# make the plot
city_or_plot |>
  ggplot(aes(x = estimate, y = city_state)) +
  geom_point() +
  geom_errorbar(aes(xmin = conf.low, xmax = conf.high)) +
  geom_vline(xintercept = 1, linetype = "dashed") +
  labs(
    x = "Adjusted odds ratio (Male vs Female)",
    y = "City",
    title = "OR for homicide being solved (Male vs Female victims) by city"
  )
```
The Washington Post articles discussed how half of murders going unsolved in the US. In this analysis, we see that most cities show adjusted odds ratios below 1, indicating that homicides with male victims are less likely to be solved than those with female victims after adjusting for age and race. Cities such as Baltimore show particularly low odds of resolving cases involving male victims, which are consistent with the findings of the article. Overall, these results suggest significant sex-based disparities in justice outcomes across U.S. cities.


**Problem 2**

```{r, echo=FALSE, message=FALSE, warning=FALSE}

# Load data
data("weather_df")

# Function to compute R^2 and beta1*beta2
fit_one_boot = function(df) {
  fit = lm(tmax ~ tmin + prcp, data = df)

  r2 = glance(fit)$r.squared

  coefs = tidy(fit)
  beta1 = coefs |> filter(term == "tmin") |> pull(estimate)
  beta2 = coefs |> filter(term == "prcp") |> pull(estimate)

  tibble(
    r2 = r2,
    beta1_beta2 = beta1 * beta2
  )
}

# 5000 bootstrap samples
set.seed(123)
boot_results =
  weather_df |>
  bootstrap(n = 5000) |>
  mutate(est = map(strap, fit_one_boot)) |>
  select(-strap) |>
  unnest(est)

# Plot R^2 distribution
boot_results |>
  ggplot(aes(x = r2)) +
  geom_histogram(bins = 30) +
  labs(
    title = expression("Bootstrap Distribution of " ~ R^2),
    x = expression(R^2),
    y = "Count"
  )

# Plot beta1*beta2 
boot_results |>
  ggplot(aes(x = beta1_beta2)) +
  geom_histogram(bins = 30) +
  labs(
    title = expression("Bootstrap Distribution of " ~ beta[1] %*% beta[2]),
    x = expression(beta[1] %*% beta[2]),
    y = "Count"
  )

# 95% CI for R^2 and beta1*beta2
ci_r2 =
  boot_results |>
  summarize(
    low = quantile(r2, 0.025),
    high = quantile(r2, 0.975)
  )

ci_beta =
  boot_results |>
  summarize(
    low = quantile(beta1_beta2, 0.025),
    high = quantile(beta1_beta2, 0.975)
  )

ci_r2
ci_beta

```


Using 5000 bootstrap samples, I found that the model predicting maximum temperature using minimum temperature and precipitation consistently performed very well. The distribution of the r-squared values was centered around 0.94, meaning the model explains about 94% of the variation in maximum temperature. The 95% confidence interval for r^2 was narrow (0.934 to 0.947), showing that this estimate is very stable and would likely be similar if new samples were collected.

I also looked at the product of the two regression coefficients (beta1 and beta2), which helps us understand how the combined effects of minimum temperature and precipitation relate to maximum temperature. The bootstrap distribution for this statistic was around -0.006, and the 95% confidence interval (–0.00823 to –0.00371) was entirely negative. This suggests that higher precipitation slightly reduces the positive relationship between minimum and maximum temperature. 

**Problem 3**
```{r, echo=FALSE, message=FALSE, warning=FALSE}

birth_raw = read_csv("data/birthweight.csv")

birth_df =
  birth_raw |>
  clean_names() |>
  mutate(
    babysex = factor(babysex, levels = c(1, 2),
                     labels = c("male", "female")),
    malform = factor(malform, levels = c(0, 1),
                     labels = c("absent", "present")),
    mrace = factor(mrace),
    frace = factor(frace)
  ) |>
  drop_na()

# check missingness 
birth_df |>
  summarize(across(everything(), ~ sum(is.na(.))))

# Proposed model for birthweight

mod_prop =
  lm(
    bwt ~ babysex + bhead + blength + gaweeks +
      ppbmi + wtgain + smoken + momage + mrace,
    data = birth_df
  )

summary(mod_prop)

# Residuals vs fitted plot

birth_aug =
  birth_df |>
  add_predictions(mod_prop) |>
  add_residuals(mod_prop)

birth_aug |>
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = 0.3) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(
    x = "Fitted birthweight (grams)",
    y = "Residuals",
    title = "Residuals vs fitted values: proposed birthweight model"
  )


# Model A: length + gestational age (main effects only)
mod_A = lm(bwt ~ blength + gaweeks, data = birth_df)

# Model B: Full model
mod_B = lm(bwt ~ babysex * bhead * blength, data = birth_df)

#Cross-validation

set.seed(123)

cv_df =
  crossv_mc(birth_df, n = 100, test = 0.2)

cv_res =
  cv_df |>
  mutate(
    train = map(train, as_tibble),
    test  = map(test,  as_tibble),

    fit_prop = map(train, ~ lm(
      bwt ~ babysex + bhead + blength + gaweeks +
        ppbmi + wtgain + smoken + momage + mrace,
      data = .x
    )),
    fit_A = map(train, ~ lm(
      bwt ~ blength + gaweeks,
      data = .x
    )),
    fit_B = map(train, ~ lm(
      bwt ~ babysex * bhead * blength,
      data = .x
    )),

    rmse_prop = map2_dbl(fit_prop, test, ~ rmse(.x, .y)),
    rmse_A    = map2_dbl(fit_A,    test, ~ rmse(.x, .y)),
    rmse_B    = map2_dbl(fit_B,    test, ~ rmse(.x, .y))
  )

# RMSE
rmse_long =
  cv_res |>
  select(starts_with("rmse_")) |>
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse"
  ) |>
  mutate(model = recode(
    model,
    rmse_prop = "Proposed model",
    rmse_A    = "Model A: length + gaweeks",
    rmse_B    = "Model B: sex*head*length"
  ))

# summary table of prediction error
rmse_summary =
  rmse_long |>
  group_by(model) |>
  summarize(
    mean_rmse = mean(rmse),
    sd_rmse   = sd(rmse),
    .groups = "drop"
  )

rmse_summary

# Boxplot of RMSE by model
rmse_long |>
  ggplot(aes(x = model, y = rmse)) +
  geom_boxplot() +
  coord_flip() +
  labs(
    x = "Model",
    y = "RMSE (cross-validated)",
    title = "Cross-validated prediction error across birthweight models"
  )
```


I chose predictors based on what is known to influence birthweight in real life, such as the baby’s size at birth, how long the pregnancy lasted, the mother’s weight and weight gain during pregnancy, smoking, age, and race. When I checked the residuals from this model, they were mostly centered around zero (and were normal distributions) suggesting that the model fits the data. 

Then I compared this model to two simpler models using cross-validated prediction error. One basic model used only birth length and gestational age, and another used head circumference, length, sex, and all the interactions. The simpler models did not predict birthweight as well as our chosen model, and the proposed model had the lowest prediction error. This means that including important maternal health and behavior factors helps improve how accurately we can predict birthweight, and therefore our model does a better job of capturing what influences a baby’s size at birth.
